# -*- coding: utf-8 -*-
"""ML Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IkiTYytMUZlFM4auyl7g_bJyyqJoeek8

# Health News Tweet Clustering using Kmeans

Reuters
https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter


Preprocessing:
- Remove the tweet id and timestamp
- Remove any word that starts with the symbol @ e.g. @AnnaMedaris
- Remove any hashtag symbols e.g. convert #depression to depression
- Remove any URL
- Convert every word to lowercase


Results:
- Create a table for 5 different values for K / SSE / Size of each cluster
"""

# Libraries
import pandas as pd
import numpy as np
import requests
from IPython.display import display, HTML
from re import M
from pandas.compat.numpy.function import SUM_DEFAULTS
from tables import index
import random
from random import sample

def pretty_print(df):
    return display( HTML( df.to_html().replace("\\n","<br>") ) )

# Data Preprocessing
def preprocess(lines):
  text_clean = ""

  for line in lines:
    linetemp = ""
    line = line[50:]  # Remove header id/timestamps
    line = line.replace("#", "") # Remove hashtags
    line = line.replace("'s", "") # Remove possessive for better keyword matching

    words = line.split(" ")
    # Check and filter through each word
    for word in reversed(words):
      if "@" in word:   # Remove @ user tags
          words.remove(word)

      if "http" in word:  # Remove links, all have http but otherwise regex would be better
        words.remove(word)

      if word == "RT":  # Remove retweets 
        words.remove(word)
      #print(words)

    # Put the words back together into a sentence after removing
    line = ' '.join(words)    
    line = line.lower()   # covert every word to lowercase

    # Further punctuation preprocessing
    line = line.replace(":", "") 
    line = line.replace("'", "") 
    line = line.replace("-", "") 
    line = line.replace(",", "") 
    line = line.replace(".", "") 
    line = line.replace("?", "") 
    line = line.replace("!", "")
    line = line.replace("(", "")  
    line = line.replace(")", "") 
    line = line.replace('"', '') 
    line = line.replace("/", " ")   # separate slashes into two keywords

    #print(line)
    text_clean = text_clean + line + "\n"

  #print(text_clean)
  text_clean2 = text_clean.split("\n")

  print(text_clean2)
  return text_clean2

# Distance Function
def distance(tweet1, tweet2):
  # Determine similarity between 2 tweets
  twt1 = tweet1.split(" ")
  twt2 = tweet2.split(" ")
  same = []
  diff = []
  
  for word in twt1:
    if word in twt2:
      if word not in same:
        same.append(word)
    else:
      if word not in diff:
        diff.append(word)

  for word in twt2:
    if word in twt1:
      if word not in same:
        same.append(word)
    else:
      if word not in diff:
        diff.append(word)

  similarity = len(same) / (len(same) + len(diff))
  return 1 - similarity

def updateCentroids(k, distancesdf, text_clean2, centroid, assign):
  # Create a list of cluster indexes for each cluster (list of k lists)
  cluster_indexes = [ [] for _ in range(k) ]
  for n in range(len(distancesdf.index)):
    for m in range(k):
      if assign[n] == ("Dist C" + str(m + 1)):
        cluster_indexes[m].append(n)

  # Convert from index to row/column name for df manipulation 
  cluster_indexes2 = [ [] for _ in range(k) ]
  for x in range(len(cluster_indexes)):
    for y in range(len(cluster_indexes[x])):
      temp = cluster_indexes[x][y]
      cluster_indexes2[x].append(text_clean2[temp])


  # Sum of Squared Error
  # Sum of of K clusters' distance between mean centroid and each x
  sse = 0
  for a in range(len(cluster_indexes2)): # Loop through each k clusters
    for b in cluster_indexes2[a]:  # For each node in the cluster
      #print(centroid[a])
      #print(b)
      sse = sse + distance(centroid[a], b)  # Add dist betw centroid and b


  tempdf = []
  new_centroids = []
  # Calculate new k centers
  # Determine the tweet with the least distance to the other tweets (Tweet with min distance sum)
  for i in range(k):  
      # Keep only tweets of the ith cluster
      tempdf = distancesdf[cluster_indexes2[i]]
      tempdf = tempdf.loc[cluster_indexes2[i]]
      #display(tempdf)

      sum = tempdf.sum(axis=0)  
      #display(sum)
      new_centroids.append(sum.idxmin())

  print(new_centroids)
  return new_centroids, sse, cluster_indexes

from os import XATTR_REPLACE
def kmeans(k, text_clean2, distancesdf):
  # Create Centroids, assign random centroids k
  centroids = []
  column_titles = [] 

  #for x in range(k):
    #centroids.append(text_clean2[randint(0, 99)])   
    #column_titles.append("Dist C" + str(x + 1))
  #print(centroids)
  #print(column_titles)

  # Make certain the same tweet cannot be more than one centroid
  rand_c = sample(range(299), k)
  for x in range(k):
    centroids.append(text_clean2[rand_c[x]])   
    column_titles.append("Dist C" + str(x + 1))
  

  # Create Cluster Distance Table
  interpointdf = pd.DataFrame(0, columns= column_titles, index= text_clean2[:300])
  #display(interpointdf)

  # Starting Criteria
  stop = False
  iterations = 0
  oldassign = pd.Series()
  print("STARTING LOOP")
  print(centroids)

  ######### MAIN KMEANS LOOP ###########
  while stop !=True:
    iterations += 1

    # Distance Calculation 
    for tweet in interpointdf.index:
      for x in range(k):
        interpointdf.at[tweet,column_titles[x]] = distance(tweet,centroids[x])

    # Cluster Assignment, find the cluster with the min distance from point
    assign = interpointdf.idxmin(axis="columns")
    #print(assign)

    # Check if Cluster assignments have changed
    stop = assign.equals(oldassign)
    
    # Update Centroids
    centroids, sse, cluster_indexes = updateCentroids(k, distancesdf, text_clean2, centroids, assign)
    oldassign = assign
  ######### MAIN KMEANS LOOP ###########

  #print(sse)
  #print(iterations)
  #print(cluster_indexes)
  return cluster_indexes, sse

if __name__ == '__main__':
  url = "https://raw.githubusercontent.com/emilc-jpg/Datasets/main/reuters_health.txt"
  req = requests.get(url)
  tweets_raw = req.text  # string of entire txt file 

  print(tweets_raw, file = open("reuters_health.txt", "w")) 
  tweetsf = open("reuters_health.txt", "r+")
  lines = tweetsf.readlines()

  print(lines[0])
  print(len(lines)) # 4720 tweets
  tweetsf.close() 

  # Preprocess Data
  tweets_data = preprocess(lines)

  # Calculate DF of dist of tweet to every other tweet
  distancesdf = pd.DataFrame(0, columns= tweets_data[:300], index= tweets_data[:300])
  for tweet1 in tweets_data[:300]:
    for tweet2 in tweets_data[:300]:
      distancesdf.at[tweet1,tweet2] = distance(tweet1,tweet2) 
  #display(distancesdf)

  # Display Kmeans Results
  kvalues = [2, 3, 5, 10, 20]
  clusters_display = ["", "", "", "", ""]

  resultsdf = pd.DataFrame()
  resultsdf["Value of K"] = kvalues
  resultsdf["SSE"] = [0, 0, 0, 0, 0]
  resultsdf["Size of Each Cluster"] = clusters_display

  for x in range(len(kvalues)):
    tempstring = ""
    clusters_assign, sse = kmeans(kvalues[x], tweets_data, distancesdf)
    
    for y in range(len(clusters_assign)):
      tempstring = tempstring + "\n" + str(y+1) + ": " + str(len(clusters_assign[y])) + " tweets"     
    #print(tempstring)

    resultsdf.at[x,"SSE"] = sse
    resultsdf.at[x,"Size of Each Cluster"] = tempstring

  pretty_print(resultsdf)
