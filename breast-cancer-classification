#####################################################################################################################
# Emily Chen
# CS 6375 Machine Learning
#   Assignment 2: Neural Network Analysis
#   This is a starter code in Python 3.6 for a neural network.
# Breast Cancer Classification using Neural Networks
#####################################################################################################################
#!pip install plot_keras_history
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
import sklearn
import functools as ft
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import OneHotEncoder
from sklearn import metrics
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.models import Model
from plot_keras_history import plot_history
import matplotlib.pyplot as plt
from IPython.display import display

# Set display option for pandas df
pd.set_option("display.max_rows", 500)
pd.set_option("display.max_columns", 500)
pd.set_option("display.width", 500)
np.random.seed(0)   # Random seed for consistency in testing


class NeuralNet:
    def __init__(self, dataFile, header=True):
        self.attribute_names = ["Class", "age", "menopause", "tumor-size", "inv-nodes", "node-caps", "deg-malig",
                           "breast", "breast-quad", "irradiat"]
        self.raw_input = pd.read_csv(dataFile, names=self.attribute_names)  # Read as dataframe


    # TODO: Write code for pre-processing the dataset, which would include
    # standardization, normalization,
    #   categorical to numerical, etc
    def preprocess(self):
        self.processed_data = self.raw_input
        display(self.processed_data.head(5))

        #---
        # Remove missing values (missing values denoted by "?")
        # Convert ? to NaN, remove, and then drop & reorder new index (proper indeces for one-hot df merging to align)
        self.processed_data.replace(to_replace="?", value=np.NAN, inplace=True)
        self.processed_data = self.processed_data.dropna().reset_index(drop=True) 
        #print(len(self.processed_data))    # (286 -> 277 values)

        #---
        # Class Recoding - [no recurrence, recurrence]
        self.processed_data.replace(to_replace="no-recurrence-events", value=0, inplace=True)
        self.processed_data.replace(to_replace="recurrence-events", value=1, inplace=True)
        #---
        # age Recoding - [10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99]
        #print(self.processed_data["age"].unique()) # 6 unique
        self.processed_data.replace(to_replace="10-19", value=15, inplace=True)
        self.processed_data.replace(to_replace="20-29", value=25, inplace=True)
        self.processed_data.replace(to_replace="30-39", value=35, inplace=True)
        self.processed_data.replace(to_replace="40-49", value=45, inplace=True)
        self.processed_data.replace(to_replace="50-59", value=55, inplace=True)
        self.processed_data.replace(to_replace="60-69", value=65, inplace=True)
        self.processed_data.replace(to_replace="70-79", value=75, inplace=True)
        self.processed_data.replace(to_replace="80-89", value=85, inplace=True)
        self.processed_data.replace(to_replace="90-99", value=95, inplace=True)
        #---
        # tumor-size Recoding - [0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59]
        #print(self.processed_data["tumor-size"].unique()) # 11 unique
        self.processed_data.replace(to_replace="0-4", value=2, inplace=True)
        self.processed_data.replace(to_replace="5-9", value=7, inplace=True)
        self.processed_data.replace(to_replace="10-14", value=12, inplace=True)
        self.processed_data.replace(to_replace="15-19", value=17, inplace=True)
        self.processed_data.replace(to_replace="20-24", value=22, inplace=True)
        self.processed_data.replace(to_replace="25-29", value=27, inplace=True)
        self.processed_data.replace(to_replace="30-34", value=32, inplace=True)
        self.processed_data.replace(to_replace="35-39", value=37, inplace=True)
        self.processed_data.replace(to_replace="40-44", value=42, inplace=True)
        self.processed_data.replace(to_replace="45-49", value=47, inplace=True)
        self.processed_data.replace(to_replace="50-54", value=52, inplace=True)
        self.processed_data.replace(to_replace="55-59", value=57, inplace=True)
        #---
        # inv-nodes Recoding - [0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39]
        #print(self.processed_data["inv-nodes"].unique()) # 7 unique
        self.processed_data.replace(to_replace="0-2", value=1, inplace=True)
        self.processed_data.replace(to_replace="3-5", value=4, inplace=True)
        self.processed_data.replace(to_replace="6-8", value=7, inplace=True)
        self.processed_data.replace(to_replace="9-11", value=10, inplace=True)
        self.processed_data.replace(to_replace="12-14", value=11, inplace=True)
        self.processed_data.replace(to_replace="15-17", value=16, inplace=True)
        self.processed_data.replace(to_replace="18-20", value=19, inplace=True)
        self.processed_data.replace(to_replace="21-23", value=21, inplace=True)
        self.processed_data.replace(to_replace="24-26", value=23, inplace=True)
        self.processed_data.replace(to_replace="27-29", value=27, inplace=True)
        self.processed_data.replace(to_replace="30-32", value=31, inplace=True)
        self.processed_data.replace(to_replace="33-35", value=34, inplace=True)
        self.processed_data.replace(to_replace="36-39", value=37, inplace=True)
        #---
        # node-caps/irradiat Recoding - [no, yes]
        self.processed_data.replace(to_replace="no", value=0, inplace=True)
        self.processed_data.replace(to_replace="yes", value=1, inplace=True)
        #---
        # breast Recoding - [left, right]
        self.processed_data.replace(to_replace="left", value=0, inplace=True)
        self.processed_data.replace(to_replace="right", value=1, inplace=True)
        #---
        # menopause Recoding - [lt40, ge40, premeno]
        #print(self.processed_data["menopause"].unique()) # 3 unique
        # 1. set encoder. 2. get fit transform matrix. 3. obtain different labels as np array. 4. create df
        enc = OneHotEncoder(categories="auto", handle_unknown="ignore")
        encarray = enc.fit_transform(self.processed_data[['menopause']]).toarray()
        menopause_types = enc.categories_
        menopause_types = np.array(menopause_types).ravel()
        tempdf = pd.DataFrame(encarray, columns=menopause_types)
        self.processed_data = self.processed_data.drop(['menopause'], axis=1)
        #---
        # breast-quad Recoding - [left-up, left-low, right-up,	right-low, central]
        #print(self.processed_data["breast-quad"].unique()) # 5 unique
        enc2 = OneHotEncoder(categories="auto", handle_unknown="error")
        encarray2 = enc2.fit_transform(self.processed_data[['breast-quad']]).toarray()
        breastquad_types = enc2.categories_
        breastquad_types = np.array(breastquad_types).ravel()
        tempdf2 = pd.DataFrame(encarray2, columns=breastquad_types)
        self.processed_data = self.processed_data.drop(['breast-quad'], axis=1)

        # ---
        # Merge them all together
        dflist = [self.processed_data, tempdf, tempdf2]
        df_final = dflist[0].join(dflist[1:])
        self.processed_data = df_final
        display(self.processed_data.head(5))

        ## Scale values
        #s = StandardScaler()
        #self.processed_data = pd.DataFrame(s.fit(self.processed_data).fit_transform(self.processed_data))
        #display(self.processed_data.head(5))

        return 0

    # TODO: Train and evaluate models for all combinations of parameters
    # specified. We would like to obtain following outputs:
    #   1. Training Accuracy and Error (Loss) for every model
    #   2. Test Accuracy and Error (Loss) for every model
    #   3. History Curve (Plot of Accuracy against training steps) for all
    #       the models in a single plot. The plot should be color coded i.e.
    #       different color for each model

    # Specify neural network
    def getModel(params):
        activation_type = params[0] 
        num_layers = int(params[1]) 

        
        if num_layers == 2:
          model = Sequential()
          model.add(Dense(20, activation=activation_type))
          model.add(Dropout(0.2))
          model.add(Dense(10, activation=activation_type))
          model.add(Dropout(0.2))
          model.add(Dense(1, activation="sigmoid"))   # sigmoidal activation for output class
          return model

        elif num_layers == 3:
          model = Sequential()
          model.add(Dense(20, activation=activation_type))
          model.add(Dropout(0.2))
          model.add(Dense(20, activation=activation_type))
          model.add(Dropout(0.2))
          model.add(Dense(10, activation=activation_type))
          model.add(Dropout(0.2))
          model.add(Dense(1, activation="sigmoid"))
          return model


    def train_evaluate(self):
        ncols = len(self.processed_data.columns)
        nrows = len(self.processed_data.index)
        X = self.processed_data.iloc[:, 1:ncols]
        y = self.processed_data.iloc[:, 0]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
        
        # Scale values. Not scaling target
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Undersample majority & oversample minority
        # currently 70:30 ratio, want to train model on more balanced distribution   
        #oversamp = RandomOverSampler(sampling_strategy=0.5) 
        #undersamp = RandomUnderSampler(sampling_strategy=0.8)  
        #X_train, y_train = oversamp.fit_resample(X_train, y_train) # oversample minority
        #X_train, y_train = undersamp.fit_resample(X_train, y_train)# combine with undersample majority


        # Below are the hyperparameters that you need to use for model evaluation
        activations = ['logistic', 'tanh', 'relu']
        learning_rate = [0.01, 0.1]
        max_iterations = [100, 200] # also known as epochs
        num_hidden_layers = [2, 3]

        hyper_cols = ["activation", "hidden_layers", "learning_rate", "max_epochs", 
                      "training_acc","test_acc", "training_error", "test_error"]
        hyperresults_df = pd.DataFrame(columns=hyper_cols)

        hyperList = np.array([
            ["sigmoid", 2, 0.01, 100],
            ["sigmoid", 2, 0.01, 200],
            ["sigmoid", 2, 0.1, 100],
            ["sigmoid", 2, 0.1, 200],
            ["sigmoid", 3, 0.01, 100],
            ["sigmoid", 3, 0.01, 200],
            ["sigmoid", 3, 0.1, 100],
            ["sigmoid", 3, 0.1, 200],  
            ["tanh", 2, 0.01, 100],
            ["tanh", 2, 0.01, 200],
            ["tanh", 2, 0.1, 100],
            ["tanh", 2, 0.1, 200],
            ["tanh", 3, 0.01, 100],
            ["tanh", 3, 0.01, 200],
            ["tanh", 3, 0.1, 100],
            ["tanh", 3, 0.1, 200],
            ["relu", 2, 0.01, 100],
            ["relu", 2, 0.01, 200],
            ["relu", 2, 0.1, 100],
            ["relu", 2, 0.1, 200],
            ["relu", 3, 0.01, 100],
            ["relu", 3, 0.01, 200],
            ["relu", 3, 0.1, 100],
            ["relu", 3, 0.1, 200]
        ])

        #---
        # Create the neural network and be sure to keep track of the performance metrics
        histories = []
        for params in hyperList:
          lrate = float(params[2])
          max_epochs = int(params[3])

          model = NeuralNet.getModel(params)
          optimizer = tf.keras.optimizers.Adam(learning_rate=lrate)
          model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=['accuracy'])
          
          temphistory = model.fit(X_train, y_train, epochs=max_epochs, batch_size=64)   # Training
          histories.append(temphistory)
          #model.summary()
          
          train_error = temphistory.history.get('loss')[-1]
          train_acc = temphistory.history.get('accuracy')[-1]
          print("train error, train accuracy", train_error, train_acc)

          test_error, test_acc = model.evaluate(X_test, y_test)  # Evaluate test
          print("test error, test accuracy", test_error,test_acc)

          # Save stats into df
          mod_results = [params[0], params[1], params[2], params[3], train_acc, test_acc, train_error, test_error]
          hyperresults_df.loc[len(hyperresults_df)] = mod_results

        # Print table
        display(hyperresults_df)



        #---
        # Plot the model history for each model in a single plot
        # model history is a plot of accuracy vs number of epochs
        # you may want to create a large sized plot to show multiple lines in a same figure.

        #plot_history(
        #    histories,
        #    show_standard_deviation=False
        #)

        plt.figure(figsize=(10,6), dpi=200)
        for hist in histories:
          plt.plot(hist.history['accuracy'])
        
        plt.title('Hyperparameter Model Accuracy')
        plt.ylabel=('accuracy')
        plt.xlabel=('epochs')
        plt.legend(['s2_0.01_100', 's2_0.01.200', 's2_0.1_100','s2_0.1_200',
                    's3_0.01_100', 's3_0.01.200', 's3_0.1_100','s3_0.1_200',
                    't2_0.01_100', 't2_0.01.200', 't2_0.1_100','t2_0.1_200',
                    't3_0.01_100', 't3_0.01.200', 't3_0.1_100','t3_0.1_200',
                    'r2_0.01_100', 'r2_0.01.200', 'r2_0.1_100','r2_0.1_200',
                    'r3_0.01_100', 'r3_0.01.200', 'r3_0.1_100','r3_0.1_200'], loc='lower right')
        plt.show()


        #fig,ax = plt.subplots(1,1 figsize=(10,6))
        #ax.plot(np.sqrt(lr_model_history.history['acc']), 'r', label='train')
        #ax.set_xlabel(r'Accuracy', fontside=20)
        #ax.set_ylabel(r'Epoch', fontsize=20)
        #ax.legend()
        #ax.tick_params(labelsize=15)


        return 0



if __name__ == "__main__":
    # Put in path to your file
    data_filepath = "https://raw.githubusercontent.com/emilc-jpg/Datasets/main/breast-cancer.data"

    neural_network = NeuralNet(data_filepath)
    neural_network.preprocess()
    neural_network.train_evaluate()

